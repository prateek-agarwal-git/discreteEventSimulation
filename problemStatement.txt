Simulation Analysis of a Web Application


Important Deadlines
Simulation Program Design (Classes, Program Logic - events, event handlers,etc)  due: 
March 10th, 2021. Submit a slide presentation, not a report.


Simulation Program Functional Demo: March 17th.
* Submit all your main code and scripts, and input files if any. 
* Code should be well-commented
* Code should produce a clean, somewhat readable trace.
* In your demo you will have to point out (reason-through) a few examples (from trace output) of why you think the code is correct
* You need not show metrics calculation at this point.




final Submission (Code, Analysis in slides format) due: March 20th,2021: 


* Submit a file called assmt2_final_name1_name2.tar. Tarball should have:
   * All code (please delete junk files), scripts, all input files used.
   * Slides which show the graphs, observations and conclusions




Assignment is to be done by TWO team members.
Overall Goal of the Assignment


In this assignment you will continue the study of the performance of a Web Application, this time through a discrete event simulation model. You will take advantage of the power of simulation analysis and model behaviours that are difficult to capture using theoretical queuing systems and Markov chain models. You will write a program, and then run it to ask various interesting questions. For all metrics you will perform proper statistical soundness analysis.


Detailed Specifications


Write a simulation program in any general purpose simulation language (except C. It is not a good habit in today’s times to program in C - please use every opportunity to program in object oriented languages. C++, Java or python is fine.)


System Characteristics
Your system should have the characteristics of the Web server environment that you measured. The following is recommended, you can add/modify.


* Multi-core server machine - C > 1
* Assume thread-to-core affinity. Once a thread is “assigned” to a core, it will remain there - NOT USEFUL. IMPLICITLY ASSUMED.
* Multi-threaded Web server - SAME AS PREVIOUS TWO.

* Thread-per task model - until max is reached, after which queuing for threads starts. MAX HERE IS THE NUMBER OF THREADS.  A SINGLE QUEUE IS TO BE ASSUMED. 
 * Round-robin scheduling, with context-switching overhead. CONTEXT SWITCHING OVERHEAD WILL BE A CONSTANT PARAMETER. (CAN BE VARIED IF REQUIRED TO GET VARIED RESULTS).
* Request time-outs - QUEUE SIZE ON SERVER IS FIXED. WHENEVER A NEW REQUEST ARRIVES WHEN THE QUEUE IS FULL, IT CAN BE ASSUMED THAT THE REQUEST WILL NOT BE SERVICED AND IT IS TIMED OUT.
* Users are in a standard closed loop - issue request, wait for response, think then issue request again.
   * Think time should have a mode not close to zero - don’t assume exponential think time
---issue request - ARRIVAL RATE. think time - UNIFORM DISTRIBUTION in the range (e.g 3-5 minute)
* Have options of various request execution time distributions, such as - constant, uniform, exponential.-- SERVICE TIME DISTRIBUTIONS
* Request timeout should also have a well-chosen distribution- ??
   * Have some sort of a minimum, and then a variable component - QUALITATIVELY SAME AS THINK time.


Performance Metrics
Metrics/graphs will be similar to what you measured for assignment. One set of your graphs should be “measured vs MVA model vs simulation model” comparison charts, which you should get by using the parameters of your measured system. 


* Average Response Time  vs number of users
   * Generate confidence intervals for this metric. Point estimates (averages of estimates from independent runs) are ok for the remaining metrics
* Throughput, Goodput, Badput  vs Number of users
   * Badput = rate of completion of requests which were already timed out.
   * Goodput = rate of completion of requests that were not timed out
   * Throughput = goodput + badput
* Request Drop rate vs number of users
* Average core utilization vs Number of users
* Some additional graphs representing your own curiosity regarding system performance vs some system parameter




Ensure that the  transient is determined and discarded in each of your simulation runs. You can do this informally, or follow Welch’s procedure. 


Note that the analysis expected in this Assignment is open-ended, and simply plotting the charts suggested above may not guarantee ‘full marks’. You may even code different architectures of Web servers, various scheduling policies, overload control etc. Or ask good “What-If” questions from the above set up. 
Self-Proposed DES Project
Students who are excited about some other ‘systems’ they may  be working on in other parts of their M.Tech. programme, or are in general interested in, may submit a proposal to carry out a simulation analysis of such a system. The proposal should contain the details of the system they plan to study (e.g. see above how the web server is described), and the questions of interest that they hope to answer through the simulation. 
* Feb 22nd: Draft Proposal submission
* March 7th: Proposal finalization after feedback/discussion
* Rest of the dates are same.

next request = currenttime + randomValue
            =3 + 2 = 5
3,4,5,8
